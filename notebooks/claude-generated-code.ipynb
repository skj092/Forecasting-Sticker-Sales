{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909099a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T07:13:42.457724Z",
     "iopub.status.busy": "2025-01-07T07:13:42.457197Z",
     "iopub.status.idle": "2025-01-07T07:13:42.465055Z",
     "shell.execute_reply": "2025-01-07T07:13:42.464062Z"
    },
    "papermill": {
     "duration": 0.012774,
     "end_time": "2025-01-07T07:13:42.466558",
     "exception": false,
     "start_time": "2025-01-07T07:13:42.453784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml \n",
    "models:\n",
    "  - n_estimators: 1000\n",
    "    learning_rate: 0.03\n",
    "    max_depth: 10\n",
    "    min_child_weight: 5\n",
    "    subsample: 0.8\n",
    "    colsample_bytree: 0.8\n",
    "    gamma: 0.0\n",
    "    reg_alpha: 0.0\n",
    "    reg_lambda: 1.0\n",
    "    random_state: 42\n",
    "\n",
    "  - n_estimators: 1000\n",
    "    learning_rate: 0.03\n",
    "    max_depth: -1\n",
    "    num_leaves: 128\n",
    "    min_child_samples: 20\n",
    "    subsample: 0.8\n",
    "    colsample_bytree: 0.8\n",
    "    reg_alpha: 0.0\n",
    "    reg_lambda: 1.0\n",
    "    verbosity: -1\n",
    "    random_state: 42\n",
    "\n",
    "  - iterations: 1000\n",
    "    learning_rate: 0.03\n",
    "    depth: 10\n",
    "    loss_function: MAPE\n",
    "    random_state: 42\n",
    "    silent: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203d2c78",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-07T07:13:42.471982Z",
     "iopub.status.busy": "2025-01-07T07:13:42.471662Z",
     "iopub.status.idle": "2025-01-07T07:15:28.501867Z",
     "shell.execute_reply": "2025-01-07T07:15:28.500763Z"
    },
    "papermill": {
     "duration": 106.034902,
     "end_time": "2025-01-07T07:15:28.503840",
     "exception": false,
     "start_time": "2025-01-07T07:13:42.468938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train_data shape: (230130, 6)\n",
      "Initial test_data shape:  (98550, 5)\n",
      "Final train shape: (220643, 18) y shape: (220643,)\n",
      "Test shape: (98550, 18)\n",
      "\n",
      "=== Training XGB ===\n",
      "XGB - Mean CV MAPE (original scale): 0.9220%\n",
      "\n",
      "=== Training LGBM ===\n",
      "LGBM - Mean CV MAPE (original scale): 0.9430%\n",
      "\n",
      "=== Training CatBoost ===\n",
      "CatBoost - Mean CV MAPE (original scale): 1.0162%\n",
      "\n",
      "Sample Submission Preview:\n",
      "       id   num_sold\n",
      "0  230130  30.332567\n",
      "1  230195  20.669648\n",
      "2  230194  24.938250\n",
      "3  230193  23.670749\n",
      "4  230192  30.608132\n",
      "\n",
      "Submission file 'submission_ensemble.csv' created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ================= CONFIG =================\n",
    "# Hyperparams - adjust or tune for best results\n",
    "with open('config.yaml') as yf:\n",
    "    config = yaml.safe_load(yf)\n",
    "xgb_params, lgb_params, cat_params = config['models']\n",
    "\n",
    "\n",
    "# =============== READ DATA ===============\n",
    "path = Path(\"/kaggle/input/playground-series-s5e1\")\n",
    "train_data = pd.read_csv(path / 'train.csv', parse_dates=['date'])\n",
    "test_data = pd.read_csv(path / 'test.csv', parse_dates=['date'])\n",
    "sample_sub = pd.read_csv(path / 'sample_submission.csv')\n",
    "\n",
    "print(\"Initial train_data shape:\", train_data.shape)\n",
    "print(\"Initial test_data shape: \", test_data.shape)\n",
    "\n",
    "# =============== CLEAN & PREPARE TRAIN ===============\n",
    "# 1) Drop duplicates & missing target\n",
    "train_data.drop_duplicates(inplace=True)\n",
    "train_data.dropna(subset=['num_sold'], inplace=True)\n",
    "\n",
    "# 3) Sort by date for time-series logic\n",
    "train_data = train_data.sort_values('date').reset_index(drop=True)\n",
    "test_data = test_data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# =============== FEATURE ENGINEERING ===============\n",
    "\n",
    "\n",
    "def create_date_features(df):\n",
    "    # Basic date parts\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['Year'] = df['date'].dt.year\n",
    "    df['Quarter'] = df['date'].dt.quarter\n",
    "    df['Month'] = df['date'].dt.month\n",
    "    df['Day'] = df['date'].dt.day\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "    # Cyclical Features\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['Day'] / 365.0)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['Day'] / 365.0)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['Month'] / 12.0)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['Month'] / 12.0)\n",
    "    df['year_sin'] = np.sin(2 * np.pi * df['Year'] / 7.0)\n",
    "    df['year_cos'] = np.cos(2 * np.pi * df['Year'] / 7.0)\n",
    "\n",
    "    # Group Calculation\n",
    "    df['Group'] = (df['Year'] - 2010) * 48 + df['Month'] * 4 + df['Day'] // 7\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_data = create_date_features(train_data)\n",
    "test_data = create_date_features(test_data)\n",
    "\n",
    "# 4) Lag & Rolling (grouped)\n",
    "#    We'll do lag_7 and rolling_7 for train only\n",
    "group_cols = ['country', 'store', 'product']\n",
    "train_data['lag_7'] = train_data.groupby(group_cols)['num_sold'].shift(7)\n",
    "train_data['rolling_7'] = (\n",
    "    train_data.groupby(group_cols)['num_sold'].shift(1).rolling(7).mean()\n",
    ")\n",
    "#\n",
    "# # Drop rows that are NaN due to lag/rolling\n",
    "train_data.dropna(subset=['lag_7', 'rolling_7'], inplace=True)\n",
    "\n",
    "# 5) Log transform the target\n",
    "train_data['num_sold'] = np.log1p(train_data['num_sold'])\n",
    "\n",
    "\n",
    "# 6) Some columns exist in train but not in test (lag_7, rolling_7),\n",
    "#    so let's add placeholder columns to the test data so XGB won't complain:\n",
    "for col in ['lag_7', 'rolling_7']:\n",
    "    if col not in test_data.columns:\n",
    "        test_data[col] = 0.0  # simple placeholder\n",
    "\n",
    "# 7) Drop date column if not needed\n",
    "train_data.drop('date', axis=1, inplace=True, errors='ignore')\n",
    "test_data.drop('date', axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# 8) Drop 'id' from train; keep test IDs for submission\n",
    "if 'id' in train_data.columns:\n",
    "    train_data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "test_ids = None\n",
    "if 'id' in test_data.columns:\n",
    "    test_ids = test_data['id'].copy()\n",
    "    test_data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# Identify numeric & categorical columns\n",
    "num_cols = train_data.select_dtypes(include=np.number).drop(\n",
    "    columns=['num_sold']).columns.tolist()\n",
    "cat_cols = train_data.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Encode categoricals\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "    label_encoders[col] = le\n",
    "    if col in test_data.columns:\n",
    "        test_data[col] = le.transform(test_data[col])\n",
    "\n",
    "# Final features & target\n",
    "X = train_data.drop(['num_sold'], axis=1)\n",
    "y = train_data['num_sold']\n",
    "X_test_final = test_data.copy()\n",
    "\n",
    "print(\"Final train shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Test shape:\", X_test_final.shape)\n",
    "\n",
    "\n",
    "# =============== CV FUNCTIONS ===============\n",
    "\n",
    "\n",
    "def cross_val_xgb(X, y, X_test, params):\n",
    "    mape_scores = []\n",
    "    test_preds_list = []\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_valid_log = model.predict(X_valid)\n",
    "    score = mean_absolute_percentage_error(y_valid, y_pred_valid_log)\n",
    "    mape_scores.append(score)\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test)\n",
    "    # Convert from log scale to original scale for final preds\n",
    "    test_preds_list.append(np.expm1(y_test_pred_log))\n",
    "\n",
    "    avg_test_preds = np.mean(test_preds_list, axis=0)\n",
    "    return np.mean(mape_scores), avg_test_preds\n",
    "\n",
    "\n",
    "def cross_val_lgbm(X, y, X_test, params):\n",
    "    mape_scores = []\n",
    "    test_preds_list = []\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_valid_log = model.predict(X_valid)\n",
    "    score = mean_absolute_percentage_error(y_valid, y_pred_valid_log)\n",
    "    mape_scores.append(score)\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test)\n",
    "    test_preds_list.append(np.expm1(y_test_pred_log))\n",
    "\n",
    "    avg_test_preds = np.mean(test_preds_list, axis=0)\n",
    "    return np.mean(mape_scores), avg_test_preds\n",
    "\n",
    "\n",
    "def cross_val_catboost(X, y, X_test, params):\n",
    "    mape_scores = []\n",
    "    test_preds_list = []\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=False)\n",
    "\n",
    "    y_pred_valid_log = model.predict(X_valid)\n",
    "    score = mean_absolute_percentage_error(y_valid, y_pred_valid_log)\n",
    "    mape_scores.append(score)\n",
    "\n",
    "    y_test_pred_log = model.predict(X_test)\n",
    "    test_preds_list.append(np.expm1(y_test_pred_log))\n",
    "\n",
    "    avg_test_preds = np.mean(test_preds_list, axis=0)\n",
    "    return np.mean(mape_scores), avg_test_preds\n",
    "\n",
    "# =============== TRAIN & PREDICT ===============\n",
    "\n",
    "\n",
    "print(\"\\n=== Training XGB ===\")\n",
    "xgb_cv_score, xgb_test_preds = cross_val_xgb(\n",
    "    X, y, X_test_final, xgb_params)\n",
    "print(f\"XGB - Mean CV MAPE (original scale): {xgb_cv_score * 100:.4f}%\")\n",
    "\n",
    "print(\"\\n=== Training LGBM ===\")\n",
    "lgb_cv_score, lgb_test_preds = cross_val_lgbm(\n",
    "    X, y, X_test_final, lgb_params)\n",
    "print(f\"LGBM - Mean CV MAPE (original scale): {lgb_cv_score * 100:.4f}%\")\n",
    "\n",
    "print(\"\\n=== Training CatBoost ===\")\n",
    "cat_cv_score, cat_test_preds = cross_val_catboost(\n",
    "    X, y, X_test_final, cat_params)\n",
    "print(f\"CatBoost - Mean CV MAPE (original scale): {cat_cv_score * 100:.4f}%\")\n",
    "\n",
    "# =============== ENSEMBLE ===============\n",
    "# Simple unweighted average of the three model predictions (already in original scale)\n",
    "ensemble_test_preds = (xgb_test_preds + lgb_test_preds + cat_test_preds) / 3.0\n",
    "\n",
    "# If one model is consistently better in local CV, do a weighted average:\n",
    "ensemble_test_preds = 0.5*lgb_test_preds + 0.3*xgb_test_preds + 0.2*cat_test_preds\n",
    "\n",
    "# =============== SUBMISSION ===============\n",
    "if test_ids is None:\n",
    "    print(\"No 'id' column found in test data. Cannot create submission with IDs.\")\n",
    "else:\n",
    "    submission = pd.DataFrame(\n",
    "        {'id': test_ids, 'num_sold': ensemble_test_preds})\n",
    "    print(\"\\nSample Submission Preview:\")\n",
    "    print(submission.head())\n",
    "\n",
    "    submission.to_csv(\"submission_ensemble.csv\", index=False)\n",
    "    print(\"\\nSubmission file 'submission_ensemble.csv' created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e359979",
   "metadata": {
    "papermill": {
     "duration": 0.001932,
     "end_time": "2025-01-07T07:15:28.508064",
     "exception": false,
     "start_time": "2025-01-07T07:15:28.506132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 108.993135,
   "end_time": "2025-01-07T07:15:29.329256",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-07T07:13:40.336121",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
